{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c286f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install awscli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1d00f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!aws configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea84b6a9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!aws s3 ls s3://spacenet-dataset/AOIs/AOI_1_Rio/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f6974",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p datasets/AOIs/AOI_1_Rio && aws s3 cp s3://spacenet-dataset/AOIs/AOI_1_Rio/processedData/processedBuildingLabels.tar.gz datasets/AOIs/AOI_1_Rio/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905b519d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Unzip the main folder\n",
    "!tar -xvzf datasets/AOIs/AOI_1_Rio/processedBuildingLabels.tar.gz -C datasets/AOIs/AOI_1_Rio/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3bc6d5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Unzip 3band satellite images\n",
    "!tar -xvzf datasets/AOIs/AOI_1_Rio/processedBuildingLabels/3band.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4ba3f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Unzip geojson containing labels satellite images\n",
    "!tar -xvzf datasets/AOIs/AOI_1_Rio/processedBuildingLabels/vectordata/geojson.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e055928d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ee5eb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "images_folder = \"3band\"\n",
    "labels_folder = \"geojson\"\n",
    "print(len([name for name in os.listdir(images_folder)]), \"satellite images\")\n",
    "print(len([name for name in os.listdir(labels_folder)]), \"geojson labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61310dcc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28ac321",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from create_data_loaders import create_data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59cc388",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "target_dir = os.getcwd() + \"/Rio\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "def merge_folders(source1, source2, destination, rename_duplicates=True):\n",
    "    os.makedirs(destination, exist_ok=True)\n",
    "\n",
    "    def copy_files(source):\n",
    "        for filename in os.listdir(source):\n",
    "            src_path = os.path.join(source, filename)\n",
    "            if os.path.isfile(src_path):\n",
    "                dest_path = os.path.join(destination, filename)\n",
    "\n",
    "                if os.path.exists(dest_path) and rename_duplicates:\n",
    "                    # Add a suffix to avoid overwriting\n",
    "                    name, ext = os.path.splitext(filename)\n",
    "                    counter = 1\n",
    "                    while os.path.exists(dest_path):\n",
    "                        new_name = f\"{name}_{counter}{ext}\"\n",
    "                        dest_path = os.path.join(destination, new_name)\n",
    "                        counter += 1\n",
    "\n",
    "                shutil.copy2(src_path, dest_path)\n",
    "\n",
    "    copy_files(source1)\n",
    "    copy_files(source2)\n",
    "\n",
    "    print(\"Finish merging\")\n",
    "\n",
    "merge_folders(images_folder, labels_folder, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d41ba24",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import RioDataset\n",
    "import create_data_loaders\n",
    "importlib.reload(RioDataset)\n",
    "importlib.reload(create_data_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5846eed7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from create_data_loaders import create_data_loaders\n",
    "train_loader, val_loader, test_loader, full_dataset = create_data_loaders(target_dir, batch_size=32, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, random_seed=42)\n",
    "print(train_loader)\n",
    "print(val_loader)\n",
    "print(test_loader)\n",
    "print(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0712fac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "from models.unet.model import UNet\n",
    "model = UNet(num_classes=1, in_channels=3)  # Adjust in_channels based on your .tiff files\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device: \", device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd2e8c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from DiceBCELoss import DiceBCELoss\n",
    "criterion = DiceBCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3345d8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Trainig Loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    loop = tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for images, masks in loop:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = torch.sigmoid(outputs) > 0.5\n",
    "        acc = (pred == masks).float().mean()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_acc += acc.item()\n",
    "\n",
    "        loop.set_postfix({\n",
    "            \"loss\": loss.item(),\n",
    "            \"acc\": acc.item()\n",
    "        })\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    dice_score = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Calculate Dice coefficient\n",
    "            pred = torch.sigmoid(outputs) > 0.5\n",
    "            dice_score += (2 * (pred * masks).sum()) / ((pred + masks).sum() + 1e-8)\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_train_acc = train_acc / len(train_loader)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, '\n",
    "          f'Val Loss: {val_loss/len(val_loader):.4f}, Dice: {dice_score/len(val_loader):.4f}')\n",
    "    \n",
    "    # Update learning rate based on validation losp\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'building_segmentation_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a395a585",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del model, train_loader, val_loader, outputs, loss\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "import gc\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
